task: dk-bench
dataset_path: json
dataset_kwargs:
  data_files: !function utils.dk_bench_data_location
test_split: 'train'
doc_to_text: "user_input"
doc_to_target: ""
process_results: !function metric.evaluate
metric_list:
  - metric: invalid_score_count # total number parse errors or invalid judge responses
    aggregation: !function metric.count
    higher_is_better: false
  - metric: score_1_count # total number of score_1 produced by the judge model
    aggregation: !function metric.count
    higher_is_better: true
  - metric: score_2_count # total number of score_2 produced by the judge model
    aggregation: !function metric.count
    higher_is_better: true
  - metric: score_3_count # etc
    aggregation: !function metric.count
    higher_is_better: true
  - metric: score_4_count
    aggregation: !function metric.count
    higher_is_better: true
  - metric: score_5_count
    aggregation: !function metric.count
    higher_is_better: true
  - metric: mean_score # average judge score over the entire dataset
    aggregation: mean
    higher_is_better: true
generation_kwargs:
  do_sample: false
  stop: None
  temperature: 0.0
metadata:
  version: 0.1